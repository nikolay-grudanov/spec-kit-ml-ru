description = "Уточнение ML-специфичных требований проекта с автоматической проверкой окружения"

[[prompt]]
role = "system"
content = """
Сначала проанализируй информацию о текущем окружении:

{{$CHECK_ENV_OUTPUT}}

На основе этой информации задай пользователю следующие уточняющие вопросы для ML проекта:

## Метрики производительности модели:
1. Какая основная метрика для оценки модели? (accuracy, F1, precision, recall, MAE, RMSE, ROC-AUC)
2. Есть ли дополнительные метрики для мониторинга?
3. Какой минимальный порог метрики считается приемлемым для production?

## Качество данных:
1. Какова ожидаемая схема данных? (типы столбцов, допустимые значения)
2. Как обрабатывать missing values? (удаление, заполнение средним, ML imputation)
3. Какие проверки качества данных требуются перед обучением?
4. Как предотвратить data leakage? (split ДО preprocessing)

## Стратегия валидации:
1. Какое разделение train/val/test? (по умолчанию 70/15/15)
2. Требуется ли cross-validation? (для малых датасетов < 10k)
3. Нужна ли stratification? (для классификации с несбалансированными классами)

## Безопасность и конфиденциальность:
1. Содержат ли данные персональную информацию? (GDPR compliance)
2. Требуется ли анонимизация данных?
3. Кто имеет доступ к данным и моделям?

## Управление версиями:
1. Какая система версионирования моделей? (MLflow, DVC, git-based tracking)
2. Как отслеживать эксперименты? (MLflow, Weights & Biases)
3. Какая стратегия для rollback моделей в production?

Ответы на эти вопросы будут использованы для генерации детальной спецификации.
"""

[[prompt]]
role = "user"
content = "{{args}}"

scripts = [
    'bash -c "python3 .ml-spec/scripts/check_environment.py --json"'
]
